Bienvenue dans Forensic-Tools ou ntfs_stratigraphy ! 
(Il aura souvent changé de noms..)

Voici un petit helper permettant de comprendre la structure du projet. 

###########################################################################################################

Le répertoire 'parser' contient les modules de principaux de traitements - les 3 parsers de fichiers 
système. Ils sont tous 3 utilisables de façon indépendante (mais nécessite d'avoir déjà extrait les 
fichiers en questions), n'hésitez pas à jouer avec le --help de chaque script, bien que ce ne soit pas 
très sorcier en termes d'arguments à spécifier. Le script main.py prend en argument une image forensique 
et fait les étapes d'extraction de $MFT/$Boot/$Bitmap > parsing > enregistrement au format CSV 
automatiquement. Il demande simplement de spécifier l'offset du début de la partition qui nous intéresse.

Dans les ressources, il y a quelques fonctions utiles types extraire les bits de certains bytes, 
transformation du Windows FILETIME en qqch de lisible, etc. Il contient également le script process.py, 
utilisé lors des tests sur les volume live. Ce script effectue des actions et extrait les fichiers
systèmes à certains temps définis (début, tous les X actions si spécifié, fin) et les transforme directement
au format CSV à l'aide des 3 parsers. 

	- bitmap.py : traitement du fichier $Bitmap (pas vraiment utilisé finalement)
	- MFT.py : traitement du fichier $MFT (il manque des attributs, comme certains n'étaient pas
		utiles dans le cadre de la stratigraphie - type attribut $EA, mais ce sera complété
		prochainement pour que ce soit un parser complètement fonctionnel et pas visé
		analyse par stratigraphie)
	- boot.py : traitement du fichier $Boot, enregistre les informations du volume dans un fichier texte
		en plus du CSV.
	- process.py : pour simuler les actions utilisateur. L'exécution se fait depuis une console 
		       PowerShell avec les droits d'administrateur ! Ce script est très fouilli, il évoluait
		       à chaque nouvelle simulation. 
	- main.py : automatise l'extraction/parsing/traitement de ces fichiers à partir d'une
		    image forensique donnée en paramètre.

Le répertoire contient également ce qui est nécessaire de la librairie sleuthkit. Dans sleuthkit/bin, il
est possible de retrouver les formes exécutables des commandes de TSK. 
https://www.sleuthkit.org/sleuthkit/download.php

Sans grande surprise, le sous-dossier nsrl contient la base de données NSRL téléchargée depuis :
https://www.nist.gov/itl/ssd/software-quality-group/national-software-reference-library-nsrl/nsrl-download

Note : les répertoires data et output étaient utilisés à des fins de tests !

###########################################################################################################

Le répertoire 'treatments' contient divers notebook ayant été utilisés pour observer les traces
(en plus des .twb) et créer les graphes retrouvés dans le LaTeX. 

!! Note importante !!
Si vous souhaitez run ces notebook, il faut faire attention à la méthode utilisée pour
enregistrer les graphes au format .pdf (write_image) qui utilise une librairie Javascript (plotly.js). 
Il y a peut être possibilité qu'il faille la retélécharger (https://github.com/plotly/plotly.js) si 
elle produit une erreur type (--plotly.js wrong URL or file path) ou faire attention à ce que le projet ou
les notebook se situent dans un chemin ne contenant ni accents, ni espaces (allez savoir pourquoi).

Avec cela ça devrait correctement marcher.

Également pour les notebook des images d'ordinateur : les graphes étant très lourd, ils ne sont pas 
'runnés'. Si vous voulez jouer avec il faut décommenter le #fig.show() - mais attention j'ai personnellement
eu des problèmes où je perdais l'entièreté du notebook lorsqu'il y avait trop de graphes (-> trop lourd)

	- D'ailleurs à ce jour, le notebook du cas CT est déchu (RIP). Heureusement quelques résultats
	se trouvent dans le cahier de laboratoire.

###########################################################################################################

Les scripts app.py et rules.py permettent l'automatisation de l'analyse par stratigraphie. Rules.py s'occupe
de faire les traitements appliqués dans les notebook lors de l'étude et de flaguer les actions utilisateur
d'intérêt + les fichiers systèmes sur la base de la NSRL et du chemin dans lesquels se trouvent les fichiers. 
Il enregistre les évènements détectés au format txt (events.txt) et produits des graphes en PDF ou HTML. La
version interactive permet d'aller chercher sur le graphe où l'action a été détectée. 

App.py avait été créé pour produire un rapport de synthèse interactif - mais finalement pas si utile que ça.
Il ne marche plus à ce jour. RIP


 